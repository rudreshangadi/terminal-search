#!/home/yash/anaconda3/bin/python
#This is a simple webscraper to scrap summary of any topic from
#https://en.wikipedia.org
#Assumptions :
#1. The search is well-formed i.e. it can be searched directly searched in
#   wikipedia
#   eg. 'harvard university' will work
#2. Firefox is installed
#3. geckodriver is installed


# importing modules
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support.expected_conditions import staleness_of
from selenium.webdriver.common.keys import Keys
import sys
import requests
from bs4 import BeautifulSoup

# handling parameters obtained from terminal
def search():
    input = sys.argv[1:]
    input = ' '.join(input)
    return input

# setting up a headless browser
def options():
    options = Options()
    options.set_headless(headless = True)
    return options

# setting path to log unavoidable logs
def log_path():
    path_file = sys.argv[0]
    path_dir = '/'.join(path_file.split('/')[:-1])
    log_path = path_dir + '/log/geckodriver.log'
    return log_path

# navigate to page via selenium
def extract_page(search, log_path):

    # opening site in web browser
    browser = webdriver.Firefox(firefox_options = options(), log_path= \
    log_path())
    browser.get('https://en.wikipedia.org')

    # searching in seach box
    elem = browser.find_element(by = 'name', value = 'search')
    elem.send_keys(search() + Keys.RETURN)

    # to handle loading of new pages in selenium
    # through the method of checking staleness of page
    # only for firefox
    timeout = 30
    old_page = browser.find_element_by_tag_name('html')
    WebDriverWait(browser, timeout).until(staleness_of(old_page))

    # getting the url of the new page
    url = browser.current_url

    # closing browser
    browser.close()

    return url

# opens a socket and returns page content
def scraper(addr):

    # opening a socket and ignoring proxies
    session = requests.Session()
    session.trust_env = False
    source = session.get(addr)

    # reading the data in a variable
    html = source.text
    soup = BeautifulSoup(html, 'html.parser')
    return soup

# finds the appropriate section of page
def print_part(scrap):
    # finding the appropriate section and printing
    paras = scrap.find_all('p')
    req_para = paras[0]
    return req_para.text

# main
addr = extract_page(search, log_path)
scrap = scraper(addr)
print('\nSummary:\n', print_part(scrap))
